# Open NSL Avatar â€“ Prosjektoversikt

## ğŸ¯ Executive Summary

**Open NSL Avatar** er et lokalt kjÃ¸rende system som konverterer ekte videoer av norsk tegnsprÃ¥k (NSL) til hÃ¸ypresisjons animasjoner pÃ¥ MetaHuman-avatarer i Unreal Engine.

### Kjerneverdier
- âœ… **Autentisitet**: Motion capture fra virkelige tegnsprÃ¥kutÃ¸vere, ikke generativ AI
- âœ… **Presisjon**: State-of-the-art tracking for hender, ansikt og kropp
- âœ… **Skalerbarhet**: Designet for 9000+ tegn
- âœ… **Lokalt**: Ingen cloud-avhengigheter (GDPR-vennlig)
- âœ… **Ã…pent**: Ikke lÃ¥st til Ã©n leverandÃ¸r eller plattform

### Hva dette IKKE er
- âŒ Ikke tekst-til-tegnsprÃ¥k (generativ)
- âŒ Ikke cloud-tjeneste
- âŒ Ikke "fake signing" - dette er motion truth

---

## ğŸ“Š Datasett

**Kilde**: minetegn.no  
**Antall**: 9000+ videoer (Ã©n per ord/variant)  
**Lokasjon**: `D:\tegnsprÃ¥k\minetegn_videos\`  
**Manifest**: `D:\tegnsprÃ¥k\minetegn_manifest_app_excel.csv`

### Video-karakteristikk
- NÃ¸ytral bakgrunn (blÃ¥/svart)
- Konsistent framing og startposisjon
- Varierende opplÃ¸sning (eldre = lavere kvalitet)
- Stabil kameravinkel
- Hender starter i nÃ¸ytral posisjon (â†’ bra for auto-trimming)

---

## ğŸ—ï¸ Arkitektur (overordnet)

```
Video (MP4)
    â†“
Preprosessering (normalisering, ROI, trimming)
    â†“
Multi-pass tracking (pose + hender + ansikt)
    â†“
Temporal stabilisering + fusjon
    â†“
Canonical NSL skeleton + face features
    â†“
MetaHuman Control Rig mapping
    â†“
Unreal AnimSequence + Curves
    â†“
Triggerbar avatar per ord
```

**Alt kjÃ¸rer lokalt pÃ¥ PC med NVIDIA GPU.**

---

## ğŸ“š Dokumentasjonsstruktur

### Kjernefilter (mÃ¥ leses i rekkefÃ¸lge)
1. **[01_REQUIREMENTS.md](01_REQUIREMENTS.md)** â€“ MÃ¥l, scope, leveranser
2. **[02_ARCHITECTURE.md](02_ARCHITECTURE.md)** â€“ Teknisk arkitektur og dataflyt
3. **[03_TECH_STACK.md](03_TECH_STACK.md)** â€“ VerktÃ¸y, biblioteker, valg
4. **[04_TRACKING_PIPELINE.md](04_TRACKING_PIPELINE.md)** â€“ Tracking-modeller og algoritmer
5. **[05_DATABASE_SCHEMA.md](05_DATABASE_SCHEMA.md)** â€“ Supabase struktur
6. **[06_OUTPUT_FORMATS.md](06_OUTPUT_FORMATS.md)** â€“ Data-formater og konvensjoner
7. **[07_METAHUMAN_MAPPING.md](07_METAHUMAN_MAPPING.md)** â€“ Control Rig mapping (NSL-optimalisert)
8. **[08_UNREAL_INTEGRATION.md](08_UNREAL_INTEGRATION.md)** â€“ Unreal import og AnimSequence
9. **[09_IMPLEMENTATION_PHASES.md](09_IMPLEMENTATION_PHASES.md)** â€“ Utviklingsplan (Phase 1, 2, 2.1)
10. **[10_CURSOR_PROMPTS.md](10_CURSOR_PROMPTS.md)** â€“ Master prompts for hver fase

### StÃ¸ttedokumenter
- **[TECH_DECISIONS.md](TECH_DECISIONS.md)** â€“ Evaluering av valg + alternativer
- **[QUALITY_SCORING.md](QUALITY_SCORING.md)** â€“ Kvalitetsscore-algoritmer
- **[BATCH_OPTIMIZATION.md](BATCH_OPTIMIZATION.md)** â€“ Batch-strategier for 9000+ filer
- **[FAILURE_MODES.md](FAILURE_MODES.md)** â€“ Kjente problemer og mitigering

---

## ğŸš€ Quick Start (for utviklere)

### Forutsetninger
- Windows PC med kraftig NVIDIA GPU (CUDA)
- Python 3.11+
- Docker Desktop (for Supabase local)
- Disk: minimum 500 GB (video + cache + eksport)

### Utviklingsfaser
**Phase 1** (MVP): Basis pipeline + MediaPipe tracking + disk/database  
**Phase 2**: SOTA tracking (OpenPose + MediaPipe + temporal stabilisering)  
**Phase 2.1**: MetaHuman Control Rig mapping + Unreal Python import

### FÃ¸rste steg
1. Les [01_REQUIREMENTS.md](01_REQUIREMENTS.md)
2. GÃ¥ gjennom [03_TECH_STACK.md](03_TECH_STACK.md)
3. Bruk [10_CURSOR_PROMPTS.md](10_CURSOR_PROMPTS.md) for implementering i Cursor

---

## ğŸ“ BruksomrÃ¥der (langsiktig)

- **Undervisning**: Interaktive lÃ¦ringsapper
- **Kiosker**: Selvbetjening med tegnsprÃ¥k
- **Spill**: Tilgjengelige NPCer
- **Web**: Three.js-avatarer
- **XR**: VR/AR tegnsprÃ¥ktolking
- **Forskning**: Ã…pne NSL-ressurser med hÃ¸y kvalitet

---

## ğŸ“– Viktige prinsipper

### Motion truth, ikke gjetning
Dette prosjektet ekstraherer ekte bevegelse fra virkelige tegnsprÃ¥kutÃ¸vere. Ingen generative modeller som "finner pÃ¥" tegn.

### Avatar-agnostisk
Selv om MetaHuman er fÃ¸rsteprioritet, er arkitekturen designet slik at:
- Canonical skeleton kan mappes til andre rigger
- Data kan brukes i andre motorer (Blender, Unity, Three.js)

### Kvalitet over kvantitet
Systemet skal:
- Detektere dÃ¥rlig tracking
- Score kvalitet per ord
- Tillate re-prosessering med bedre modeller senere

---

## ğŸ“ Status og roadmap

### Ferdigstilt (fÃ¸r Cursor)
- [x] Dataset nedlastet (9000+ videoer)
- [x] Manifest CSV laget
- [x] Konseptvalidering

### Neste (Cursor-implementering)
- [ ] Phase 1: Basis pipeline
- [ ] Phase 2: SOTA tracking
- [ ] Phase 2.1: MetaHuman integration
- [ ] Testing med 100+ ord
- [ ] Full batch-kjÃ¸ring

---

## ğŸ¤ Hvordan bidra / bruke dokumentasjonen

### For AI-assistert koding (Cursor)
Bruk **[10_CURSOR_PROMPTS.md](10_CURSOR_PROMPTS.md)** direkte som input.

### For menneskelige utviklere
Start med **[01_REQUIREMENTS.md](01_REQUIREMENTS.md)** og fÃ¸lg rekkefÃ¸lgen.

### For teknisk review
Les **[TECH_DECISIONS.md](TECH_DECISIONS.md)** for Ã¥ evaluere valg.

---

**Neste: GÃ¥ til [01_REQUIREMENTS.md](01_REQUIREMENTS.md) â†’**
