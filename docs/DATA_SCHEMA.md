# ğŸ’¾ Data Schemas

**Version**: v1 (Phase 2)
**Last Updated**: 2025-12-16

This document defines the data formats used and generated by the NSL Avatar system.

---

## ğŸ“‚ Output Structure

All processing results are stored in the `workspace/tracks/` directory. Each video processing run creates a unique UUID folder:

```
workspace/tracks/
â””â”€â”€ {uuid}/
    â”œâ”€â”€ meta.json          # High-level metadata & quality scores
    â”œâ”€â”€ tracking.parquet   # Efficient binary frame data (Time series)
    â”œâ”€â”€ tracking.jsonl.gz  # Compressed JSONL (Human readable backup)
    â””â”€â”€ visualization.mp4  # Debug video with skeletal overlay
```

---

## ğŸ“„ Metadata (`meta.json`)

Contains the summary of the processing job.

```json
{
  "word": "video_filename_without_extension",
  "filename": "original_filename.mp4",
  "video_path": "relative/path/to/video.mp4",
  "tracking_provider": "rtmpose",  // or "mediapipe"
  "frames": 150,                   // Total frames processed
  "quality_score": 0.85,           // 0.0 - 1.0 (Weighted Average)
  "issues": [                      // List of detected quality issues
    {
      "type": "low_hand_visibility",
      "severity": "warning",
      "value": 0.45
    }
  ],
  "format_version": "v1"
}
```

---

## â±ï¸ Tracking Data (`tracking.parquet` / `.jsonl`)

The core tracking data is a time-series. Each row/record represents **one frame**.

### Fields per Frame

| Field | Type | Description |
|-------|------|-------------|
| `frame_index` | int | Frame number (0-based) |
| `time_s` | float | Timestamp in seconds |
| `pose_landmarks` | List[Point] | Body points (Shoulders, Elbows, etc.) |
| `left_hand_landmarks` | List[Point] | Left hand points (21 points) |
| `right_hand_landmarks` | List[Point] | Right hand points (21 points) |
| `face_landmarks` | List[Point] | Face mesh points (468 for MP, varied for RTMPose) |
| `pose_confidence` | float | Avg confidence of body tracking |
| `left_hand_confidence` | float | Avg confidence of left hand |
| `right_hand_confidence` | float | Avg confidence of right hand |
| `face_confidence` | float | Avg confidence of face |

### Point Structure (`Landmark2D`)

Each landmark point contains:
*   `x`: Normalized X coordinate (0.0 - 1.0)
*   `y`: Normalized Y coordinate (0.0 - 1.0)
*   `confidence`: Detection confidence for this specific point (0.0 - 1.0)
*   `name`: (Optional) Keypoint name e.g., "nose", "left_wrist"

---

## ğŸ—ºï¸ Keypoint Mapping

### MediaPipe & RTMPose (COCO-WholeBody)
We normalize landmarks to a common standard where possible, but distinct topologies exist.

*   **Body**: 33 Keypoints (MediaPipe standard)
*   **Hands**: 21 Keypoints (0=Wrist, 1-4=Thumb, 5-8=Index, etc.)
*   **Face**: 
    *   MediaPipe: 468 landmarks (Dense mesh)
    *   RTMPose: 133 landmarks (COCO-WholeBody standard) - *Note: Be careful when mapping to MetaHuman, topology differs.*

---

## ğŸ”„ Future Formats (Phase 2.1)

In the next phase (Unreal Engine Integration), we will export:
*   **`.anim`**: Unreal Engine Animation Asset
*   **`.fbx`**: Retargeted Skeleton
